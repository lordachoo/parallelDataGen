#!/usr/bin/env python3
import os
import random
import threading
import argparse

__version__ = '1.1'
import json
import sys
import platform
import fcntl

# Optional psutil import for enhanced system metrics
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
from pathlib import Path
from time import time
from datetime import datetime
from multiprocessing import cpu_count

class DummyDataGenerator:
    def __init__(self, output_dir, num_files, file_size_kb, thread_count=None, node_id=0, node_count=1):
        self.output_dir = Path(output_dir)
        self.num_files = num_files
        self.file_size_bytes = file_size_kb * 1024
        self.thread_count = thread_count or cpu_count()
        self.lock = threading.Lock()
        self.node_id = node_id
        self.node_count = node_count
        self.status_file = self.output_dir / f".dummy_data_status_node{self.node_id}.json"
        self.files_created = 0
        self.start_time = None
        self.last_update_time = None
        
        # Create output directory if it doesn't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)
        # Pre-generate random data buffer
        self.data_buffer = self.generate_random_data(self.file_size_bytes)

    def generate_random_data(self, size):
        """Generate random binary data of given size"""
        return bytes(random.getrandbits(8) for _ in range(size))

    def create_file(self, file_num):
        """Create a single dummy file"""
        global_num = (file_num * self.node_count) + self.node_id
        file_path = self.output_dir / f"dummy_n{self.node_id}_{global_num}.dat"
        
        # Check if file exists (in case of overlapping runs)
        if file_path.exists():
            with self.lock:
                print(f"Warning: {file_path} already exists, skipping")
            return
        try:
            # Use a larger buffer size for better performance
            with open(file_path, 'wb', buffering=1024*1024) as f:
                f.write(self.data_buffer)
            with self.lock:
                print(f"Created {file_path} ({self.file_size_bytes/1024:.2f} KB)")
                self.files_created += 1
                if self.files_created % 10 == 0:  # Update status every 10 files
                    self.update_node_status()
        except Exception as e:
            print(f"Error creating {file_path}: {e}")

    def update_node_status(self):
        """Update this node's status file"""
        try:
            now = datetime.utcnow()
            current_time = now.isoformat()
            
            # Calculate throughput if we have previous data
            throughput = None
            if self.last_update_time and self.files_created > 0:
                time_diff = (now - self.last_update_time).total_seconds()
                if time_diff > 0:
                    data_mb = (self.file_size_bytes * 10) / (1024 * 1024)  # 10 files since last update
                    throughput = data_mb / time_diff  # MB/s
            
            status = {
                'node_id': self.node_id,
                'files_created': self.files_created,
                'percent_complete': (self.files_created / self.num_files) * 100,
                'last_update': current_time,
                'throughput_mb_s': throughput,
                'files_per_sec': self.files_created / (now - self.start_time).total_seconds() if self.start_time else None,
                'node_metadata': {
                    'node_id': self.node_id,
                    'node_count': self.node_count,
                    'thread_count': self.thread_count,
                    'file_size_kb': self.file_size_bytes / 1024,
                    'target_files': self.num_files,
                    'hostname': os.uname().nodename if hasattr(os, 'uname') else 'unknown',
                    'start_time': self.start_time.isoformat() if self.start_time else None,
                    'python_version': '.'.join(map(str, sys.version_info[:3])),
                    'platform': sys.platform,
                    'cpu_model': platform.processor(),
                    **({
                        'system_memory_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                        'available_memory_gb': round(psutil.virtual_memory().available / (1024**3), 2),
                        'cpu_cores': psutil.cpu_count(logical=False),
                        'cpu_threads': psutil.cpu_count(logical=True)
                    } if PSUTIL_AVAILABLE else {
                        'psutil_missing': True,
                        'cpu_cores': os.cpu_count() or 'unknown',
                        'cpu_threads': os.cpu_count() or 'unknown'
                    })
                }
            }
            
            self.last_update_time = now
            
            # Write to temp file first
            temp_file = self.status_file.with_suffix('.tmp')
            with open(temp_file, 'w') as f:
                json.dump(status, f, indent=2)
                f.flush()
                os.fsync(f.fileno())
            
            # Atomic rename
            os.replace(temp_file, self.status_file)
            
        except Exception as e:
            print(f"Warning: Could not update node status file - {e}")

    def get_cluster_status(self):
        """Aggregate status from all nodes"""
        status = {
            'timestamp': datetime.utcnow().isoformat(),
            'total_files': self.num_files * self.node_count,
            'file_size_kb': self.file_size_bytes / 1024,
            'nodes': {},
            'aggregate_stats': {
                'total_throughput_mb_s': 0.0,
                'total_files_per_sec': 0.0,
                'active_nodes': 0,
                'total_files_created': 0,
                'percent_complete': 0.0
            }
        }
        
        # Find all node status files
        status_files = list(self.output_dir.glob('.dummy_data_status_node*.json'))
        
        for status_file in status_files:
            try:
                with open(status_file, 'r') as f:
                    node_status = json.load(f)
                    node_id = str(node_status['node_id'])
                    status['nodes'][node_id] = node_status
                    
                    # Update aggregate stats
                    if node_status.get('throughput_mb_s') is not None:
                        status['aggregate_stats']['total_throughput_mb_s'] += node_status['throughput_mb_s']
                        if node_status.get('files_per_sec'):
                            status['aggregate_stats']['total_files_per_sec'] += node_status['files_per_sec']
                        status['aggregate_stats']['active_nodes'] += 1
                    status['aggregate_stats']['total_files_created'] += node_status['files_created']
                    
            except Exception as e:
                print(f"Warning: Could not read status file {status_file} - {e}")
        
        if status['nodes']:
            status['aggregate_stats']['percent_complete'] = round(
                (status['aggregate_stats']['total_files_created'] / status['total_files']) * 100, 1)
        
        return status

    def run(self):
        """Run the generation process with threading"""
        self.start_time = datetime.utcnow()
        self.last_update_time = self.start_time
        # Write initial status
        self.update_node_status()
        print(f"Starting generation of {self.num_files} files ({self.file_size_bytes/1024:.2f} KB each)")
        print(f"Using {self.thread_count} threads")
        
        start_time = time()
        threads = []
        
        # Distribute files across threads
        files_per_thread = self.num_files // self.thread_count
        remaining_files = self.num_files % self.thread_count
        
        for i in range(self.thread_count):
            # Calculate files for this thread
            files_to_create = files_per_thread + (1 if i < remaining_files else 0)
            if files_to_create == 0:
                continue
                
            # Create and start thread
            start_num = i * files_per_thread + min(i, remaining_files)
            t = threading.Thread(
                target=self.create_files_batch,
                args=(start_num, files_to_create)
            )
            threads.append(t)
            t.start()
        
        # Wait for all threads to complete
        for t in threads:
            t.join()
            
        total_size_gb = (self.num_files * self.file_size_bytes) / (1024**3)
        elapsed = time() - start_time
        # Final status update
        self.update_node_status()
        print(f"\nCompleted in {elapsed:.2f} seconds")
        
        # Print cluster-wide summary
        try:
            status = self.get_cluster_status()
            print(f"\nCluster-wide status:")
            print(f"- Total files created: {status['aggregate_stats']['total_files_created']}/{status['total_files']}")
            print(f"- Completion: {status['aggregate_stats']['percent_complete']}%")
            print(f"- Active nodes: {status['aggregate_stats']['active_nodes']}/{self.node_count}")
            print(f"- Total throughput: {status['aggregate_stats']['total_throughput_mb_s']:.2f} MB/s")
            print(f"- Files per second: {status['aggregate_stats']['total_files_per_sec']:.2f} files/s")
        except Exception as e:
            print(f"Warning: Could not generate cluster status - {e}")
        print(f"Total data generated: {total_size_gb:.2f} GB")
        print(f"Throughput: {total_size_gb/elapsed:.2f} GB/s")

    def create_files_batch(self, start_num, count):
        """Create a batch of files (used by threads)"""
        for i in range(start_num, start_num + count):
            self.create_file(i)

def main():
    parser = argparse.ArgumentParser(
        description='Parallel dummy data generator',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')
    parser.add_argument('output_dir', help='Output directory for dummy files')
    parser.add_argument('-n', '--num-files', type=int, default=100,
                       help='Number of files to generate per node (default: 100)')
    parser.add_argument('-s', '--size-kb', type=int, default=10240,
                       help='Size of each file in KB (default: 10240 = 10MB)')
    parser.add_argument('-t', '--threads', type=int,
                       help='Number of threads to use (default: CPU count)')
    parser.add_argument('--node-id', type=int, default=0,
                       help='Node identifier (0-based) for distributed runs')
    parser.add_argument('--node-count', type=int, default=1,
                       help='Total number of nodes in distributed run')
    
    args = parser.parse_args()
    
    generator = DummyDataGenerator(
        output_dir=args.output_dir,
        num_files=args.num_files,
        file_size_kb=args.size_kb,
        thread_count=args.threads,
        node_id=args.node_id,
        node_count=args.node_count
    )
    generator.run()

if __name__ == '__main__':
    main()
